{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047353a7",
   "metadata": {
    "_cell_guid": "8c62453f-96e3-4690-b446-66cc36944430",
    "_uuid": "2723e7ea-8f54-4ded-bc4d-654016305214",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00337,
     "end_time": "2025-11-23T07:42:47.169710",
     "exception": false,
     "start_time": "2025-11-23T07:42:47.166340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "References\n",
    "- https://www.kaggle.com/code/huikang/arc-agi-2-code-approach\n",
    "- https://www.kaggle.com/code/huikang/r1-distill-qwen-tir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a36786b",
   "metadata": {
    "_cell_guid": "fdf223c2-f8ef-467a-9dbb-cc5484786047",
    "_kg_hide-output": true,
    "_uuid": "3da124f9-ff58-4c99-8f7a-fe224f1fa647",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:42:47.176434Z",
     "iopub.status.busy": "2025-11-23T07:42:47.176211Z",
     "iopub.status.idle": "2025-11-23T07:43:10.703939Z",
     "shell.execute_reply": "2025-11-23T07:43:10.703514Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 23.532069,
     "end_time": "2025-11-23T07:43:10.704616",
     "exception": false,
     "start_time": "2025-11-23T07:42:47.172547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.18.0\n",
      "Uninstalling tensorflow-2.18.0:\n",
      "  Successfully uninstalled tensorflow-2.18.0\n",
      "Found existing installation: matplotlib 3.7.2\n",
      "Uninstalling matplotlib-3.7.2:\n",
      "  Successfully uninstalled matplotlib-3.7.2\n",
      "Found existing installation: keras 3.8.0\n",
      "Uninstalling keras-3.8.0:\n",
      "  Successfully uninstalled keras-3.8.0\n",
      "Found existing installation: scikit-learn 1.2.2\n",
      "Uninstalling scikit-learn-1.2.2:\n",
      "  Successfully uninstalled scikit-learn-1.2.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'uninstall', '--yes', 'tensorflow', 'matplotlib', 'keras', 'scikit-learn'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run(\n",
    "    [\"pip\", \"uninstall\", \"--yes\", \"tensorflow\", \"matplotlib\", \"keras\", \"scikit-learn\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88cabbe",
   "metadata": {
    "_cell_guid": "4fcbcdb6-696b-4b48-826f-c9bd42031243",
    "_uuid": "92a580b1-f0fe-46c6-a97a-9059012c30a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:10.711932Z",
     "iopub.status.busy": "2025-11-23T07:43:10.711806Z",
     "iopub.status.idle": "2025-11-23T07:43:26.712741Z",
     "shell.execute_reply": "2025-11-23T07:43:26.712244Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 16.005812,
     "end_time": "2025-11-23T07:43:26.713946",
     "exception": false,
     "start_time": "2025-11-23T07:43:10.708134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def is_on_kaggle_commit() -> bool:\n",
    "    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Batch\" and not bool(\n",
    "        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    )\n",
    "\n",
    "\n",
    "def is_on_kaggle_interactive() -> bool:\n",
    "    return os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") == \"Interactive\" and not bool(\n",
    "        os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\")\n",
    "    )\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "final_cutoff_time = start_time + (4 * 60 + 45) * 60  # 4.75 hours from start time\n",
    "cutoff_times = [\n",
    "    int(x) for x in np.linspace(final_cutoff_time, start_time + 5 * 60, 50 + 1)\n",
    "]  # 5 minutes loading time at the start\n",
    "cutoff_times.pop()\n",
    "\n",
    "os.makedirs(\"solutions\", exist_ok=True)\n",
    "\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfd5ee",
   "metadata": {
    "_cell_guid": "5c120214-e16b-45ea-a2d1-b77777bedb72",
    "_uuid": "1d6623d2-ce24-4d27-973c-142828fe4e1a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003122,
     "end_time": "2025-11-23T07:43:26.720449",
     "exception": false,
     "start_time": "2025-11-23T07:43:26.717327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Serve vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8bb14e4",
   "metadata": {
    "_cell_guid": "8783f9ec-c1d9-4110-9aaa-39d6a92b4a54",
    "_uuid": "aec8a8d8-b6af-4748-ab79-d7da2298e33d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:26.727501Z",
     "iopub.status.busy": "2025-11-23T07:43:26.727278Z",
     "iopub.status.idle": "2025-11-23T07:43:26.742070Z",
     "shell.execute_reply": "2025-11-23T07:43:26.741697Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.019117,
     "end_time": "2025-11-23T07:43:26.742606",
     "exception": false,
     "start_time": "2025-11-23T07:43:26.723489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl100k_base.tiktoken\n",
      "o200k_base.tiktoken\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ls', '/kaggle/usr/lib/pip_install_aimo3/tiktoken_encodings'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"ls\", \"/kaggle/usr/lib/pip_install_aimo3/tiktoken_encodings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58268deb",
   "metadata": {
    "_cell_guid": "7f17cd32-442e-460a-982e-7ca73668b69f",
    "_uuid": "53e3ad10-1b6f-47d8-b5de-19f556c4b7a8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:26.749764Z",
     "iopub.status.busy": "2025-11-23T07:43:26.749626Z",
     "iopub.status.idle": "2025-11-23T07:43:26.754119Z",
     "shell.execute_reply": "2025-11-23T07:43:26.753732Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.009153,
     "end_time": "2025-11-23T07:43:26.754974",
     "exception": false,
     "start_time": "2025-11-23T07:43:26.745821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs: /kaggle/working/vllm.log\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def start_vllm_server() -> subprocess.Popen[bytes]:\n",
    "    \"\"\"Start vLLM server in the background\"\"\"\n",
    "    os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    # https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#troubleshooting\n",
    "    os.environ[\"TIKTOKEN_ENCODINGS_BASE\"] = (\n",
    "        \"/kaggle/usr/lib/pip_install_aimo3/tiktoken_encodings\"\n",
    "    )\n",
    "\n",
    "    sequence_length = 65_536  # 32 * 2048\n",
    "\n",
    "    command: list[str] = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\",\n",
    "        \"/kaggle/input/gpt-oss-20b/transformers/default/1\",\n",
    "        \"--served-model-name\",\n",
    "        \"qwen3\",\n",
    "        \"--tensor-parallel-size\",\n",
    "        \"1\",\n",
    "        \"--host\",\n",
    "        \"0.0.0.0\",\n",
    "        \"--port\",\n",
    "        \"8000\",\n",
    "        \"--dtype\",\n",
    "        \"auto\",\n",
    "        \"--max-model-len\",\n",
    "        f\"{sequence_length}\",\n",
    "        \"--max-seq-len-to-capture\",\n",
    "        f\"{sequence_length}\",\n",
    "    ]\n",
    "\n",
    "    # Start the process in the background\n",
    "    with open(\"/kaggle/working/vllm.log\", \"w\") as logfile:\n",
    "        process: subprocess.Popen[bytes] = subprocess.Popen(\n",
    "            command, stdout=logfile, stderr=subprocess.STDOUT, start_new_session=True\n",
    "        )\n",
    "\n",
    "    print(\"Logs: /kaggle/working/vllm.log\")\n",
    "    return process\n",
    "\n",
    "\n",
    "# Start the server\n",
    "vllm_process: subprocess.Popen[bytes] = start_vllm_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238c6977",
   "metadata": {
    "_cell_guid": "bf725ef0-626c-4b0b-93d1-e74549a152a7",
    "_uuid": "1759af22-a458-484e-84b1-64010680e36b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:26.762345Z",
     "iopub.status.busy": "2025-11-23T07:43:26.762176Z",
     "iopub.status.idle": "2025-11-23T07:43:34.096263Z",
     "shell.execute_reply": "2025-11-23T07:43:34.095848Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 7.339129,
     "end_time": "2025-11-23T07:43:34.097194",
     "exception": false,
     "start_time": "2025-11-23T07:43:26.758065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI, Stream\n",
    "from openai.types.chat import ChatCompletion, ChatCompletionMessageParam\n",
    "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n",
    "\n",
    "# Point the client to your local vLLM server\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8000/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-local\"  # any non-empty string\n",
    "\n",
    "client: OpenAI = OpenAI(\n",
    "    base_url=os.environ[\"OPENAI_API_BASE\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    ")\n",
    "\n",
    "stop_token_ids = [\n",
    "    token_id\n",
    "    for token_id in range(200_000, 201_088)\n",
    "    if token_id not in [200005, 200006, 200007, 200008]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d278a87",
   "metadata": {
    "_cell_guid": "7506d16a-f083-47f7-a68f-c8cf2602c432",
    "_uuid": "7ffb104f-e307-4f91-926c-15177cd12f0a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:43:34.104125Z",
     "iopub.status.busy": "2025-11-23T07:43:34.103968Z",
     "iopub.status.idle": "2025-11-23T07:48:23.255137Z",
     "shell.execute_reply": "2025-11-23T07:48:23.254564Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 289.15839,
     "end_time": "2025-11-23T07:48:23.258656",
     "exception": false,
     "start_time": "2025-11-23T07:43:34.100266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncPage[Model](data=[Model(id='qwen3', created=1763884103, object='model', owned_by='vllm', root='/kaggle/input/gpt-oss-20b/transformers/default/1', parent=None, max_model_len=65536, permission=[{'id': 'modelperm-2b2094fe6ce14ac2a80ab58ef11e800a', 'object': 'model_permission', 'created': 1763884103, 'allow_create_engine': False, 'allow_sampling': True, 'allow_logprobs': True, 'allow_search_indices': False, 'allow_view': True, 'allow_fine_tuning': False, 'organization': '*', 'group': None, 'is_blocking': False}])], object='list')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for _ in range(15 * 60):\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        print(client.models.list())\n",
    "    except Exception:\n",
    "        continue\n",
    "    break\n",
    "else:\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0670eb01",
   "metadata": {
    "_cell_guid": "7726741c-26cf-4297-bb09-950ae9e812bd",
    "_uuid": "4bae670b-305a-4956-ace4-c439a613da21",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:23.265682Z",
     "iopub.status.busy": "2025-11-23T07:48:23.265513Z",
     "iopub.status.idle": "2025-11-23T07:48:23.288846Z",
     "shell.execute_reply": "2025-11-23T07:48:23.288455Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.027783,
     "end_time": "2025-11-23T07:48:23.289625",
     "exception": false,
     "start_time": "2025-11-23T07:48:23.261842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cachetools import cached, TTLCache\n",
    "from typing import Generator\n",
    "import time\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def reversed_lines(path: str, block_size: int = 4096) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Iterate over the lines of a file in reverse order (last line first),\n",
    "    without loading the entire file into memory.\n",
    "\n",
    "    Yields lines as strings (including the trailing newline if present).\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        f.seek(0, os.SEEK_END)\n",
    "        file_end = f.tell()\n",
    "\n",
    "        buffer = b\"\"\n",
    "        pos = file_end\n",
    "\n",
    "        while pos > 0:\n",
    "            # Read a block from the end going backwards\n",
    "            read_size = min(block_size, pos)\n",
    "            pos -= read_size\n",
    "            f.seek(pos, os.SEEK_SET)\n",
    "            data = f.read(read_size)\n",
    "\n",
    "            buffer = data + buffer\n",
    "            # Split into lines\n",
    "            lines = buffer.split(b\"\\n\")\n",
    "            # Keep the first (possibly incomplete) part in buffer\n",
    "            buffer = lines[0]\n",
    "            # The rest (from the end backwards) are full lines\n",
    "            for line in reversed(lines[1:]):\n",
    "                yield line.decode(\"utf-8\", errors=\"replace\") + \"\\n\"\n",
    "\n",
    "        # Finally, yield the very first line (if any)\n",
    "        if buffer:\n",
    "            yield buffer.decode(\"utf-8\", errors=\"replace\") + \"\\n\"\n",
    "\n",
    "\n",
    "@cached(cache=TTLCache(maxsize=2, ttl=10))\n",
    "def get_gpu_kv_cache_usage() -> float:\n",
    "    for line in reversed_lines(\"vllm.log\"):\n",
    "        pattern = r\"GPU KV cache usage: ([\\d.]+)%\"\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            gpu_cache_usage = float(match.group(1))\n",
    "            return gpu_cache_usage\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6922214c",
   "metadata": {
    "_cell_guid": "77510a63-d235-4a5c-b182-18a4ade735e3",
    "_uuid": "d0bcd210-101b-4f9a-9cd3-d9c656dd76c8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:23.296556Z",
     "iopub.status.busy": "2025-11-23T07:48:23.296237Z",
     "iopub.status.idle": "2025-11-23T07:48:24.152317Z",
     "shell.execute_reply": "2025-11-23T07:48:24.151870Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.860477,
     "end_time": "2025-11-23T07:48:24.153241",
     "exception": false,
     "start_time": "2025-11-23T07:48:23.292764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp: ChatCompletion = client.chat.completions.create(\n",
    "    model=\"qwen3\",  # use your served name; if not set, the model path/name vLLM shows in logs\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Reply your answer in \\\\boxed{}\"},\n",
    "        {\"role\": \"user\", \"content\": \"How many r are there in strawberry?\"},\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=1.0,\n",
    "    extra_body=dict(min_p=0.02, stop_token_ids=stop_token_ids),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d252bc",
   "metadata": {
    "_cell_guid": "3080497b-33ed-4c1d-a5fb-45ae4b8ad5d5",
    "_uuid": "07968db3-c86c-4c1d-93ca-40a8a020bd68",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:24.160869Z",
     "iopub.status.busy": "2025-11-23T07:48:24.160411Z",
     "iopub.status.idle": "2025-11-23T07:48:24.163188Z",
     "shell.execute_reply": "2025-11-23T07:48:24.162782Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007207,
     "end_time": "2025-11-23T07:48:24.163878",
     "exception": false,
     "start_time": "2025-11-23T07:48:24.156671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User asks: \"How many r are there in strawberry?\" Likely they mean count the letter 'r' in the word \"strawberry\". The word is s t r a w b e r r y. The 'r's: positions 3, 8, 9? Let's list: s(1), t(2), r(3), a(4), w(5), b(6), e(7), r(8), r(9), y(10). So there are 3 r's. But might consider \"strawberries\" plural? But question says strawberry. So answer: 3. And need to reply in \\boxed{3}.\n"
     ]
    }
   ],
   "source": [
    "print(resp.choices[0].message.reasoning_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a49cfeac",
   "metadata": {
    "_cell_guid": "2cb79916-5b27-4cb9-af64-d056ccb6bca4",
    "_uuid": "70b6ddcb-bea0-4a1c-9d66-5470a464a25e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:24.170832Z",
     "iopub.status.busy": "2025-11-23T07:48:24.170691Z",
     "iopub.status.idle": "2025-11-23T07:48:24.173142Z",
     "shell.execute_reply": "2025-11-23T07:48:24.172743Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.006687,
     "end_time": "2025-11-23T07:48:24.173794",
     "exception": false,
     "start_time": "2025-11-23T07:48:24.167107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\boxed{3}\n"
     ]
    }
   ],
   "source": [
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aba29a",
   "metadata": {
    "_cell_guid": "de5a9274-0b86-43c8-8715-f677550b7170",
    "_uuid": "7da500ca-4bf4-4af5-91d0-01a1b7611756",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003119,
     "end_time": "2025-11-23T07:48:24.180093",
     "exception": false,
     "start_time": "2025-11-23T07:48:24.176974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b11a7c1",
   "metadata": {
    "_cell_guid": "9f2711cc-d0ad-4a94-94a1-512f91eee2f9",
    "_uuid": "a84de285-e493-427b-9fd6-bbab7f89f761",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:24.187625Z",
     "iopub.status.busy": "2025-11-23T07:48:24.187170Z",
     "iopub.status.idle": "2025-11-23T07:48:24.190673Z",
     "shell.execute_reply": "2025-11-23T07:48:24.190256Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.007957,
     "end_time": "2025-11-23T07:48:24.191388",
     "exception": false,
     "start_time": "2025-11-23T07:48:24.183431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_boxed_text(text: str) -> str:\n",
    "    \"\"\"Extract text inside \\\\boxed{} from LaTeX-formatted text\"\"\"\n",
    "    import re\n",
    "\n",
    "    pattern: str = r\"oxed{(.*?)}\"\n",
    "    matches: list[str] = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def is_valid_answer_string(text: str) -> bool:\n",
    "    try:\n",
    "        if int(text) == float(text):\n",
    "            if 0 <= int(text) <= 99_999:\n",
    "                # now AIMO answers no longer need modulo\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd81ffc2",
   "metadata": {
    "_cell_guid": "07a38eab-85e1-4fe3-a0b1-c4ff0ac3ac1b",
    "_uuid": "b9a8c369-0e80-4421-b0f2-dab28170e33d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:24.198620Z",
     "iopub.status.busy": "2025-11-23T07:48:24.198469Z",
     "iopub.status.idle": "2025-11-23T07:48:24.203225Z",
     "shell.execute_reply": "2025-11-23T07:48:24.202818Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00914,
     "end_time": "2025-11-23T07:48:24.203884",
     "exception": false,
     "start_time": "2025-11-23T07:48:24.194744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "completed_question_ids: set[str] = set()\n",
    "question_id_to_counter: dict[str, Counter] = {\"\": Counter()}\n",
    "\n",
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def vote_answer(question_id: str, force_answer: bool = False) -> int | None:\n",
    "    # reads counter from global\n",
    "    counter = question_id_to_counter[question_id]\n",
    "    if force_answer and not counter:\n",
    "        completed_question_ids.add(question_id)\n",
    "        return 12453\n",
    "\n",
    "    # voting mechanism\n",
    "    modified_counter = Counter()\n",
    "    for value, count in counter.items():\n",
    "        # re-weighted because smaller answers seems to be wrong\n",
    "        # \"1.25 +\" because log(1) = 0\n",
    "        modified_counter[value] += math.log(1.25 + abs(value)) * count\n",
    "\n",
    "    total_score = sum(modified_counter.values())\n",
    "    score_list = sorted(\n",
    "        (score, counter[value], value) for value, score in modified_counter.items()\n",
    "    )\n",
    "    if force_answer:\n",
    "        print(f\"score_list | {total_score:8.1f} over {sum(counter.values())} attempts\")\n",
    "        print(f\"Current GPU usage {get_gpu_kv_cache_usage()}\")\n",
    "        for score, count, value in score_list[::-1]:\n",
    "            print(f\"{value:10}   {score:8.1f} {count:8d}\")\n",
    "        return score_list[-1][-1]\n",
    "    if score_list[-1][0] > max(50, total_score / (2 + math.log(1 + total_score))):\n",
    "        if len(score_list) == 1:\n",
    "            completed_question_ids.add(question_id)\n",
    "        else:\n",
    "            if score_list[-1][0] - score_list[-2][0] > 50:\n",
    "                # win by a certain number of points at least\n",
    "                completed_question_ids.add(question_id)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3233f500",
   "metadata": {
    "_cell_guid": "9ad61568-0a06-40c8-9fbd-02e7c5355f9d",
    "_uuid": "8da03edd-ee18-442f-a459-8b6d0308bacc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:24.211117Z",
     "iopub.status.busy": "2025-11-23T07:48:24.210979Z",
     "iopub.status.idle": "2025-11-23T07:48:24.217411Z",
     "shell.execute_reply": "2025-11-23T07:48:24.217015Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01089,
     "end_time": "2025-11-23T07:48:24.218125",
     "exception": false,
     "start_time": "2025-11-23T07:48:24.207235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def generate_solution(\n",
    "    question_text: str, question_id: str = \"\", solution_index: int = 0\n",
    ") -> str:\n",
    "    if question_id in completed_question_ids:\n",
    "        return \"\"\n",
    "    if time.time() >= cutoff_times[-1]:\n",
    "        return \"\"\n",
    "    if get_gpu_kv_cache_usage() > 65:\n",
    "        time.sleep(1)  # otherwise we will flush the queue, which is unintended\n",
    "        return \"\"\n",
    "\n",
    "    messages: list[ChatCompletionMessageParam] = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will solve the problem and return the final answer in \\\\boxed{}. The answer is expected to be an integer between 0 and 99999 inclusive. Do not guess the answer.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": question_text},\n",
    "    ]\n",
    "\n",
    "    text_response_to_save = \"\"\n",
    "    for iteration in range(3):\n",
    "        text_response = \"\"\n",
    "        breaking = False\n",
    "\n",
    "        stream: Stream[ChatCompletionChunk] = client.chat.completions.create(\n",
    "            model=\"qwen3\",  # use your served name; if not set, the model path/name vLLM shows in logs\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            stream=True,\n",
    "            extra_body=dict(min_p=0.02, stop_token_ids=stop_token_ids),\n",
    "        )\n",
    "\n",
    "        for chunk in stream:\n",
    "            chunk_text = (\n",
    "                chunk.choices[0].delta.reasoning_content\n",
    "                if hasattr(chunk.choices[0].delta, \"reasoning_content\")\n",
    "                else chunk.choices[0].delta.content\n",
    "            )\n",
    "            if chunk_text:\n",
    "                text_response += chunk_text\n",
    "            # avoid stalling the GPU\n",
    "            if get_gpu_kv_cache_usage() > 85 and solution_index % 10 == 1:\n",
    "                print(\n",
    "                    f\"generation stopped to avoid excessive GPU usage, currently {get_gpu_kv_cache_usage()}\"\n",
    "                )\n",
    "                breaking = True\n",
    "            if get_gpu_kv_cache_usage() > 95:\n",
    "                print(\n",
    "                    f\"generation due to excessive GPU usage, currently {get_gpu_kv_cache_usage()}\"\n",
    "                )\n",
    "                breaking = True\n",
    "            if question_id in completed_question_ids:\n",
    "                # stop generating if we have finalized on an answer\n",
    "                breaking = True\n",
    "            if time.time() >= cutoff_times[-1]:\n",
    "                breaking = True\n",
    "            if breaking:\n",
    "                break\n",
    "\n",
    "        messages.append({\"role\": \"assistant\", \"content\": text_response})\n",
    "        text_response_to_save += text_response\n",
    "        stream.close()\n",
    "\n",
    "        if breaking:\n",
    "            break\n",
    "\n",
    "        boxed_text = extract_boxed_text(text_response)\n",
    "        if not is_valid_answer_string(boxed_text):\n",
    "            user_follow_up = \"The answer is expected to be an integer between 0 and 99999 inclusive. Place your final answer in \\\\boxed{}. Do not guess the answer.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        elif int(boxed_text) <= 10:\n",
    "            user_follow_up = \"Are you sure that is the answer? Do not guess the answer.\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        elif iteration == 0 and get_gpu_kv_cache_usage() < 20:\n",
    "            user_follow_up = \"Have you verified your answer?\"\n",
    "            messages.append({\"role\": \"user\", \"content\": user_follow_up})\n",
    "            text_response_to_save += \"\\n===\\n\" + user_follow_up + \"\\n===\\n\"\n",
    "        else:\n",
    "            # answer found, no issues detected, continuing\n",
    "            break\n",
    "\n",
    "    boxed_text = extract_boxed_text(\n",
    "        text_response_to_save\n",
    "    )  # expected to use the full conversation\n",
    "\n",
    "    if question_id and text_response_to_save:\n",
    "        with open(f\"solutions/{question_id}/{solution_index:04d}.txt\", \"w\") as f:\n",
    "            f.write(text_response_to_save)\n",
    "\n",
    "    if is_valid_answer_string(boxed_text):\n",
    "        question_id_to_counter[question_id][int(boxed_text)] += 1\n",
    "        vote_answer(question_id)\n",
    "\n",
    "    return boxed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8cc947",
   "metadata": {
    "_cell_guid": "5a1125d9-7c4a-4bd0-9d82-d7d3305c794e",
    "_uuid": "5390f9d4-9e1e-4cae-b989-731e721daa36",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:24.225107Z",
     "iopub.status.busy": "2025-11-23T07:48:24.224868Z",
     "iopub.status.idle": "2025-11-23T07:48:25.617583Z",
     "shell.execute_reply": "2025-11-23T07:48:25.617100Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.397082,
     "end_time": "2025-11-23T07:48:25.618374",
     "exception": false,
     "start_time": "2025-11-23T07:48:24.221292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_solution(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04cf1a9d",
   "metadata": {
    "_cell_guid": "118f9b34-c37a-457b-8bf0-d5c0d3fa3338",
    "_uuid": "5ff403c4-6680-43cd-93a7-7859bacf3995",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:25.626225Z",
     "iopub.status.busy": "2025-11-23T07:48:25.626067Z",
     "iopub.status.idle": "2025-11-23T07:48:25.629797Z",
     "shell.execute_reply": "2025-11-23T07:48:25.629397Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.008405,
     "end_time": "2025-11-23T07:48:25.630457",
     "exception": false,
     "start_time": "2025-11-23T07:48:25.622052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def solve(question_text: str, question_id: str = \"\") -> int:\n",
    "    os.makedirs(f\"solutions/{question_id}\", exist_ok=True)\n",
    "    question_id_to_counter[question_id] = Counter()\n",
    "    completed_question_ids.discard(question_id)\n",
    "\n",
    "    if question_id and time.time() > cutoff_times[-1]:\n",
    "        print(\"timeout did not solve\")\n",
    "        return 12314\n",
    "\n",
    "    num_generations = 1024\n",
    "    get_gpu_kv_cache_usage()  # run once to prevent running in the first batch of execution\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=64) as executor:\n",
    "        # run in parallel\n",
    "        results = executor.map(\n",
    "            generate_solution,\n",
    "            [question_text] * num_generations,\n",
    "            [question_id] * num_generations,\n",
    "            list(range(num_generations)),\n",
    "        )\n",
    "        list(results)\n",
    "\n",
    "    final_answer = vote_answer(question_id, force_answer=True)\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06707898",
   "metadata": {
    "_cell_guid": "80a428f9-c68e-4d1a-8142-4a25ec90d1f2",
    "_uuid": "876c6b69-be5a-4055-91d4-5a1c6bf3f9e4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:25.637516Z",
     "iopub.status.busy": "2025-11-23T07:48:25.637241Z",
     "iopub.status.idle": "2025-11-23T07:48:30.256768Z",
     "shell.execute_reply": "2025-11-23T07:48:30.256277Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.623836,
     "end_time": "2025-11-23T07:48:30.257456",
     "exception": false,
     "start_time": "2025-11-23T07:48:25.633620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_list |    115.5 over 98 attempts\n",
      "Current GPU usage 0\n",
      "         2      115.5       98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa6644",
   "metadata": {
    "_cell_guid": "285433b8-11c7-4cb2-9b39-f934c2486581",
    "_uuid": "c52ec099-6ff1-43ab-a120-680aa0739024",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003804,
     "end_time": "2025-11-23T07:48:30.265291",
     "exception": false,
     "start_time": "2025-11-23T07:48:30.261487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8b8217a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T07:48:30.273233Z",
     "iopub.status.busy": "2025-11-23T07:48:30.273073Z",
     "iopub.status.idle": "2025-11-23T07:49:19.550472Z",
     "shell.execute_reply": "2025-11-23T07:49:19.549990Z"
    },
    "papermill": {
     "duration": 49.282124,
     "end_time": "2025-11-23T07:49:19.551204",
     "exception": false,
     "start_time": "2025-11-23T07:48:30.269080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_list |     61.1 over 274 attempts\n",
      "Current GPU usage 0.7\n",
      "         0       61.1      274\n",
      "score_list |     61.4 over 275 attempts\n",
      "Current GPU usage 0.3\n",
      "         0       61.4      275\n",
      "score_list |     61.4 over 275 attempts\n",
      "Current GPU usage 0.8\n",
      "         0       61.4      275\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import kaggle_evaluation.aimo_3_inference_server\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# 本地 / notebook 调试用的测试集路径\n",
    "TEST_CSV_PATH = \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\"\n",
    "\n",
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 99999, inclusive.\n",
    "def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # Unpack values（Polars 输入是一列，这里取单个样本）\n",
    "    question_id = id_.item(0)\n",
    "    problem_text: str = problem.item(0)\n",
    "\n",
    "    # 调用你前面定义好的 solve()，里面已经封装了多次采样、投票等逻辑\n",
    "    prediction = solve(problem_text, question_id=str(question_id))\n",
    "\n",
    "    # 和你原来的代码保持一致的全局状态更新\n",
    "    completed_question_ids.add(str(question_id))\n",
    "    cutoff_times.pop()\n",
    "\n",
    "    # AIMO3 要求返回一个 DataFrame，列名必须是 id / answer\n",
    "    return pl.DataFrame({\"id\": question_id, \"answer\": prediction})\n",
    "\n",
    "\n",
    "# 和官方 demo 一样的推理服务包装\n",
    "inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(\n",
    "    predict\n",
    ")\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    # 评测环境走这里：启动真正的 gRPC 服务，让官方评测脚本来喂题目\n",
    "    # 注意：你必须在脚本启动 15 分钟内调用 serve()，否则会被判失败\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # 本地 / Notebook 调试：直接用 test.csv 跑一遍，模拟评测\n",
    "    inference_server.run_local_gateway((TEST_CSV_PATH,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f0a28",
   "metadata": {
    "_cell_guid": "1e7bdc73-4aa2-47ae-a7df-52442398b911",
    "_kg_hide-input": false,
    "_uuid": "800882f9-51ae-4527-9ecc-8b7ddc41e1f9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003648,
     "end_time": "2025-11-23T07:49:19.558670",
     "exception": false,
     "start_time": "2025-11-23T07:49:19.555022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaH100",
   "dataSources": [
    {
     "databundleVersionId": 14559231,
     "sourceId": 118448,
     "sourceType": "competition"
    },
    {
     "sourceId": 280846368,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 422375,
     "modelInstanceId": 404469,
     "sourceId": 510361,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 444102,
     "modelInstanceId": 427088,
     "sourceId": 566514,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 395.593327,
   "end_time": "2025-11-23T07:49:20.278055",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-23T07:42:44.684728",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
