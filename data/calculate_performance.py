#!/usr/bin/env python3
"""Simple portfolio performance calculator for AI-Trader logs.

Usage example:
    python data/calculate_performance.py \
        --log_root data/agent_data_astock \
        --signature deepseek-chat \
        --price_file data/A_stock/merged.jsonl \
        --benchmark data/A_stock/index_daily_sse_50.json

The script scans the JSONL position log generated by the agent, reconstructs the
end-of-day portfolio value (cash + holdings) using historical prices, and prints
basic performance metrics. Optionally, it compares against a benchmark index.
"""

from __future__ import annotations

import argparse
import json
import os
from bisect import bisect_right
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Tuple


Date = datetime


@dataclass
class PriceSeries:
    dates: List[Date]
    prices: List[float]

    def get_price(self, date: Date) -> Optional[float]:
        """Return the latest available price on/before the target date."""
        if not self.dates:
            return None
        idx = bisect_right(self.dates, date) - 1
        if idx < 0:
            return None
        return self.prices[idx]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Calculate portfolio performance from agent logs")
    parser.add_argument("--log_root", default="data/agent_data_astock",
                        help="Root directory that contains <signature>/position/position.jsonl")
    parser.add_argument("--signature", required=True, help="Agent signature folder name")
    parser.add_argument("--price_file", default="data/A_stock/merged.jsonl",
                        help="Historical price file (JSONL, AlphaVantage-style)")
    parser.add_argument("--benchmark", default=None,
                        help="Optional benchmark JSON file (AlphaVantage-style)")
    parser.add_argument("--export_csv", default=None,
                        help="Optional path to export daily equity curve as CSV")
    return parser.parse_args()


def load_price_file(path: str) -> Dict[str, PriceSeries]:
    """Load merged JSONL price file into symbol -> PriceSeries mapping."""
    if not os.path.exists(path):
        raise FileNotFoundError(f"Price file not found: {path}")

    price_map: Dict[str, List[Tuple[Date, float]]] = defaultdict(list)

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            data = json.loads(line)
            meta = data.get("Meta Data", {})
            symbol = meta.get("2. Symbol") or meta.get("1. Symbol")
            if not symbol:
                continue

            time_series = data.get("Time Series (Daily)", {})
            for date_str, values in time_series.items():
                price = extract_price(values)
                if price is None:
                    continue
                date_obj = datetime.strptime(date_str, "%Y-%m-%d")
                price_map[symbol].append((date_obj, price))

    normalized: Dict[str, PriceSeries] = {}
    for symbol, entries in price_map.items():
        entries.sort(key=lambda item: item[0])
        dates = [d for d, _ in entries]
        prices = [p for _, p in entries]
        normalized[symbol] = PriceSeries(dates=dates, prices=prices)
    return normalized


def extract_price(values: Dict[str, str]) -> Optional[float]:
    """Extract the most relevant price field from a time-series record."""
    priority_keys = [
        "1. buy price",  # A-share merged format (open price)
        "1. open",
        "4. close",
        "2. high",
    ]
    for key in priority_keys:
        if key in values and values[key] not in (None, ""):
            try:
                return float(values[key])
            except ValueError:
                continue
    return None


def load_position_log(log_root: str, signature: str) -> List[Dict]:
    position_path = os.path.join(log_root, signature, "position", "position.jsonl")
    if not os.path.exists(position_path):
        raise FileNotFoundError(f"Position log not found: {position_path}")

    entries: List[Dict] = []
    with open(position_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            entries.append(json.loads(line))
    if not entries:
        raise RuntimeError("Position log is empty")
    return entries


def build_daily_snapshots(entries: List[Dict]) -> List[Tuple[Date, Dict[str, float]]]:
    """Return list of (date, positions) after the last action of each day."""
    snapshots: List[Tuple[Date, Dict[str, float]]] = []
    current_date: Optional[str] = None
    latest_positions: Optional[Dict[str, float]] = None

    for entry in entries:
        date = entry.get("date")
        positions = entry.get("positions")
        if not date or not positions:
            continue

        if current_date is None:
            current_date = date

        if date != current_date:
            # store snapshot for previous date
            snapshots.append((datetime.strptime(current_date, "%Y-%m-%d"), latest_positions or {}))
            current_date = date

        latest_positions = positions

    # append final date
    if current_date and latest_positions:
        snapshots.append((datetime.strptime(current_date, "%Y-%m-%d"), latest_positions))

    return snapshots


def compute_equity_curve(
    snapshots: List[Tuple[Date, Dict[str, float]]],
    price_data: Dict[str, PriceSeries]
) -> List[Dict[str, float]]:
    curve: List[Dict[str, float]] = []
    for date_obj, positions in snapshots:
        cash = float(positions.get("CASH", 0.0))
        holdings_value = 0.0
        missing_symbols: List[str] = []
        for symbol, amount in positions.items():
            if symbol.upper() == "CASH":
                continue
            qty = float(amount)
            if abs(qty) < 1e-9:
                continue
            price_series = price_data.get(symbol)
            if not price_series:
                missing_symbols.append(symbol)
                continue
            price = price_series.get_price(date_obj)
            if price is None:
                missing_symbols.append(symbol)
                continue
            holdings_value += qty * price
        total_value = cash + holdings_value
        curve.append(
            {
                "date": date_obj,
                "cash": cash,
                "holdings": holdings_value,
                "total": total_value,
                "missing_symbols": ",".join(missing_symbols),
            }
        )
    return curve


def compute_metrics(curve: List[Dict[str, float]]) -> Dict[str, float]:
    if len(curve) < 2:
        raise RuntimeError("Need at least two snapshots to compute performance")

    start_value = curve[0]["total"]
    end_value = curve[-1]["total"]
    total_return = (end_value / start_value) - 1.0

    daily_returns: List[float] = []
    peaks: List[float] = []
    peak = curve[0]["total"]
    max_drawdown = 0.0

    for idx in range(1, len(curve)):
        prev = curve[idx - 1]["total"]
        curr = curve[idx]["total"]
        if prev > 0:
            daily_returns.append((curr / prev) - 1.0)
        peak = max(peak, curr)
        drawdown = (curr - peak) / peak if peak > 0 else 0.0
        max_drawdown = min(max_drawdown, drawdown)
        peaks.append(peak)

    avg_daily_return = sum(daily_returns) / len(daily_returns)
    variance = sum((r - avg_daily_return) ** 2 for r in daily_returns) / max(len(daily_returns) - 1, 1)
    daily_vol = variance ** 0.5

    start_date = curve[0]["date"]
    end_date = curve[-1]["date"]
    days = (end_date - start_date).days
    years = max(days / 365.25, 1 / 365.25)
    cagr = (end_value / start_value) ** (1 / years) - 1 if start_value > 0 else 0

    return {
        "start_value": start_value,
        "end_value": end_value,
        "total_return": total_return,
        "avg_daily_return": avg_daily_return,
        "daily_vol": daily_vol,
        "max_drawdown": abs(max_drawdown),
        "cagr": cagr,
        "days": days,
        "num_points": len(curve),
    }


def load_benchmark(path: str) -> Optional[PriceSeries]:
    if not path:
        return None
    if not os.path.exists(path):
        raise FileNotFoundError(f"Benchmark file not found: {path}")

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    time_series = data.get("Time Series (Daily)", {})
    entries: List[Tuple[Date, float]] = []
    for date_str, values in time_series.items():
        price = extract_price(values)
        if price is None:
            continue
        entries.append((datetime.strptime(date_str, "%Y-%m-%d"), price))
    entries.sort(key=lambda item: item[0])
    dates = [d for d, _ in entries]
    prices = [p for _, p in entries]
    return PriceSeries(dates=dates, prices=prices)


def build_benchmark_curve(curve_dates: List[Date], benchmark: PriceSeries) -> List[float]:
    baseline = benchmark.get_price(curve_dates[0])
    if baseline is None or baseline == 0:
        return []
    return [benchmark.get_price(date) / baseline for date in curve_dates]


def export_csv(curve: List[Dict[str, float]], benchmark_curve: Optional[List[float]], path: str) -> None:
    import csv

    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        header = ["date", "cash", "holdings", "total", "missing_symbols"]
        if benchmark_curve:
            header.append("benchmark_normalized")
        writer.writerow(header)
        for idx, row in enumerate(curve):
            values = [
                row["date"].strftime("%Y-%m-%d"),
                f"{row['cash']:.2f}",
                f"{row['holdings']:.2f}",
                f"{row['total']:.2f}",
                row["missing_symbols"],
            ]
            if benchmark_curve:
                values.append(f"{benchmark_curve[idx]:.6f}")
            writer.writerow(values)


def main():
    args = parse_args()
    entries = load_position_log(args.log_root, args.signature)
    price_data = load_price_file(args.price_file)
    snapshots = build_daily_snapshots(entries)
    if not snapshots:
        raise RuntimeError("No daily snapshots extracted from position log")
    curve = compute_equity_curve(snapshots, price_data)
    metrics = compute_metrics(curve)

    print("=" * 60)
    print(f"Performance summary for {args.signature}")
    print("=" * 60)
    print(f"Data points      : {metrics['num_points']}")
    print(f"Period           : {curve[0]['date'].date()} -> {curve[-1]['date'].date()} ({metrics['days']} days)")
    print(f"Start value      : ¥{metrics['start_value']:.2f}")
    print(f"End value        : ¥{metrics['end_value']:.2f}")
    print(f"Total return     : {metrics['total_return']*100:.2f}%")
    print(f"CAGR             : {metrics['cagr']*100:.2f}%")
    print(f"Avg daily return : {metrics['avg_daily_return']*100:.3f}%")
    print(f"Daily vol        : {metrics['daily_vol']*100:.3f}%")
    print(f"Max drawdown     : {metrics['max_drawdown']*100:.2f}%")

    benchmark_curve = None
    if args.benchmark:
        benchmark_series = load_benchmark(args.benchmark)
        if benchmark_series:
            benchmark_curve = build_benchmark_curve([row["date"] for row in curve], benchmark_series)
            if benchmark_curve:
                benchmark_return = benchmark_curve[-1] - 1.0
                print("-" * 60)
                print("Benchmark comparison")
                print("-" * 60)
                print(f"Benchmark total return : {benchmark_return*100:.2f}%")
            else:
                print("⚠️  Benchmark series does not cover the portfolio period")

    if args.export_csv:
        export_csv(curve, benchmark_curve, args.export_csv)
        print(f"Daily curve exported to {args.export_csv}")


if __name__ == "__main__":
    main()
